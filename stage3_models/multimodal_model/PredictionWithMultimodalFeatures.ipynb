{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with STFT/note/scalar inputs (symbolic and audio) to combine the notebooks in a multimodal Neural Net architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install torcheval==0.0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.io.wavfile\n",
    "from scipy import signal\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os \n",
    "\n",
    "import librosa\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "\n",
    "import itertools # creating list of hyperparameter choices\n",
    "\n",
    "# for creating models:\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# for dataset split:\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "from torcheval.metrics.functional import r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your version identical with original version \n",
      "-> scipy \n",
      "-> 1.10.1\n",
      "\n",
      "\n",
      "Your version identical with original version \n",
      "-> numpy \n",
      "-> 1.21.6\n",
      "\n",
      "\n",
      "Your version identical with original version \n",
      "-> pandas \n",
      "-> 2.0.3\n",
      "\n",
      "\n",
      "For \n",
      "glob \n",
      "python version needs to fit \n",
      "-> original version 3.7.3 \n",
      "-> your version 3.8.10 (default, Jun  4 2021, 15:09:15) \n",
      "[GCC 7.5.0]\n",
      "\n",
      "\n",
      "For \n",
      "os \n",
      "python version needs to fit \n",
      "-> original version 3.7.3 \n",
      "-> your version 3.8.10 (default, Jun  4 2021, 15:09:15) \n",
      "[GCC 7.5.0]\n",
      "\n",
      "\n",
      "Your version identical with original version \n",
      "-> librosa \n",
      "-> 0.10.0.post2\n",
      "\n",
      "\n",
      "Possibly different versions: \n",
      "-> torch \n",
      "-> original version 2.0.1+cu117 \n",
      "-> your version 1.13.1\n",
      "\n",
      "\n",
      "Your version identical with original version \n",
      "-> sklearn \n",
      "-> 1.3.0\n",
      "\n",
      "\n",
      "Your version identical with original version \n",
      "-> torcheval \n",
      "-> 0.0.7\n",
      "\n",
      "\n",
      "Possibly different versions: \n",
      "-> sys \n",
      "-> original version 3.7.3 \n",
      "-> your version 3.8.10 (default, Jun  4 2021, 15:09:15) \n",
      "[GCC 7.5.0]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sys.path.insert(0, '../../stage-1/overall_used_tools')\n",
    "import requirements_check as rc\n",
    "\n",
    "import torcheval\n",
    "rc.check(sys, [scipy, np, pd, glob, os, librosa, torch , sklearn, torcheval], multimodal_prediction=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ATTENTION: When you already have in the notebook's directory the directory 'created_input_tensors' containing the files**\n",
    "- categorical_labels_tensor.pt\n",
    "- mel_tensor.pt\n",
    "- regressive_labels_tensor.pt\n",
    "- scalar_tensor.pt\n",
    "- symbolic_input_tensor.pt \n",
    "- midibert_input_tensor<br><br>\n",
    "**you can directly jump to the notebook's section '2. Create Neural Network architecture' and go on from there.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare inputs for net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Import categorical and regressive labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possibly different versions: \n",
      "-> pandas \n",
      "-> original version 1.3.5 \n",
      "-> your version 2.0.3\n",
      "\n",
      "\n",
      "For \n",
      "collections \n",
      "python version needs to fit \n",
      "-> original version 3.7.3 \n",
      "-> your version 3.8.10 (default, Jun  4 2021, 15:09:15) \n",
      "[GCC 7.5.0]\n",
      "\n",
      "\n",
      "Your version identical with original version \n",
      "-> numpy \n",
      "-> 1.21.6\n",
      "\n",
      "\n",
      "No match for your module \n",
      "matplotlib \n",
      "found in the requirements.\n",
      "\n",
      "\n",
      "Possibly different versions: \n",
      "-> sys \n",
      "-> original version 3.7.3 \n",
      "-> your version 3.8.10 (default, Jun  4 2021, 15:09:15) \n",
      "[GCC 7.5.0]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sys.path.insert(0, '../unimodal_scalarFeatures_model') # to import labels.py from another folder\n",
    "import labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/c/Schreibtisch/ba_08102022/stage3_models/multimodal_model/../unimodal_scalarFeatures_model/labels.py:139: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  label_df['MP3-Code']= label_df['MP3-Code'] + '_accompaniment' # add to each cell a string in a certain column\n",
      "/home/c/Schreibtisch/ba_08102022/stage3_models/multimodal_model/../unimodal_scalarFeatures_model/labels.py:140: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  label_df.rename(columns={'MP3-Code':'sample_id'},inplace=True) # rename column\n",
      "/home/c/Schreibtisch/ba_08102022/stage3_models/multimodal_model/../unimodal_scalarFeatures_model/labels.py:143: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  label_df.dropna(inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We import a csv file where we need the columns 'MP3-Code','sublimity','vitality','unease'. 'MP3-Code' describes the ID of each sample.\n",
      "proportion of amount of samples (out of 370) with... ...zero ...one ...two ...three labels:\n",
      "0.0 0.7753424657534247 0.2136986301369863 0.010958904109589041\n",
      "We import a csv file where we need the columns 'MP3-Code','sublimity','vitality','unease'. 'MP3-Code' describes the ID of each sample.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/c/Schreibtisch/ba_08102022/stage3_models/multimodal_model/../unimodal_scalarFeatures_model/labels.py:156: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  label_df['final_label'] = label_sample_list.tolist()\n",
      "/home/c/Schreibtisch/ba_08102022/stage3_models/multimodal_model/../unimodal_scalarFeatures_model/labels.py:162: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  label_df['sample_id'] = \"'\" + label_df['sample_id'] + \".wav'\"\n",
      "/home/c/Schreibtisch/ba_08102022/stage3_models/multimodal_model/../unimodal_scalarFeatures_model/labels.py:177: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  label_df['MP3-Code']= label_df['MP3-Code'] + '_accompaniment' # add to each cell a string in a certain column\n",
      "/home/c/Schreibtisch/ba_08102022/stage3_models/multimodal_model/../unimodal_scalarFeatures_model/labels.py:178: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  label_df.rename(columns={'MP3-Code':'sample_id'},inplace=True) # rename column\n",
      "/home/c/Schreibtisch/ba_08102022/stage3_models/multimodal_model/../unimodal_scalarFeatures_model/labels.py:181: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  label_df.dropna(inplace=True)\n",
      "/home/c/Schreibtisch/ba_08102022/stage3_models/multimodal_model/../unimodal_scalarFeatures_model/labels.py:203: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  label_df['final_label'] = label_sample_list.tolist()\n",
      "/home/c/Schreibtisch/ba_08102022/stage3_models/multimodal_model/../unimodal_scalarFeatures_model/labels.py:209: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  label_df['sample_id'] = \"'\" + label_df['sample_id'] + \".wav'\"\n"
     ]
    }
   ],
   "source": [
    "categorical_labels = labels.get_categorical_labels()\n",
    "categorical_labels.sort_index(inplace=True)\n",
    "# Change names to MP3 code:\n",
    "if '_accompaniment' in categorical_labels.index[0]: \n",
    "    categorical_labels.index = ['_'.join(i.split('_')[:-1])[1:] for i in list(categorical_labels.index)]\n",
    "    \n",
    "regres_labels = labels.get_regressive_labels()\n",
    "regres_labels.sort_index(inplace=True)\n",
    "# Change names to MP3 code:\n",
    "if '_accompaniment' in regres_labels.index[0]: \n",
    "    regres_labels.index = ['_'.join(i.split('_')[:-1])[1:] for i in list(regres_labels.index)]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Symbolic features: using note_collector as 2 D input (rows = voices, column = time aspect) of a LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your version identical with original version \n",
      "-> numpy \n",
      "-> 1.21.6\n",
      "\n",
      "\n",
      "Possibly different versions: \n",
      "-> pandas \n",
      "-> original version 1.3.5 \n",
      "-> your version 2.0.3\n",
      "\n",
      "\n",
      "Possibly different versions: \n",
      "-> mido \n",
      "-> original version 1.2.10 \n",
      "-> your version <module 'mido.version' from '/home/c/anaconda3/envs/multimodal_env/lib/python3.8/site-packages/mido/version.py'>\n",
      "\n",
      "\n",
      "Possibly different versions: \n",
      "-> sys \n",
      "-> original version 3.7.3 \n",
      "-> your version 3.8.10 (default, Jun  4 2021, 15:09:15) \n",
      "[GCC 7.5.0]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sys.path.insert(0, '../../stage2_feature_extraction/symbolicFeatureExtraction')\n",
    "import basic_info_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_symbolic_col_size(array, final_length=300):\n",
    "    \n",
    "    len_snippet = 30 # number of neighbouring notes which should be kept in shortened new array \n",
    "    number_snippets = int(final_length/len_snippet)\n",
    "    \n",
    "    piece_length = array.shape[1]\n",
    "    ind_distance = int(piece_length/number_snippets)\n",
    "    \n",
    "    current_ind = 0\n",
    "    \n",
    "    while current_ind <= (piece_length - len_snippet - 1):\n",
    "\n",
    "        snippet = array[:, current_ind:current_ind+len_snippet]\n",
    "        \n",
    "        if current_ind == 0:\n",
    "            shortened_array = snippet\n",
    "\n",
    "        else:\n",
    "            shortened_array = np.concatenate((shortened_array, snippet),axis=1)\n",
    "\n",
    "        current_ind += ind_distance # ind_distance includes len_snippet by this usage\n",
    "\n",
    "    return shortened_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = ['H_Jay-Z_HardKno', 'K_Saint-Seans_Carniva','K_Schumann_EtudesS']\n",
    "l2 = ['H_JayZ_HardKno', 'K_Saint-Saens_Carniva', 'K_Schumann_EtudeS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current midi:  P_BOBfeatBrunoMars_Nothing_accompaniment\n",
      "information extraction (and midi snippet generation) done\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcurrent midi: \u001b[39m\u001b[38;5;124m'\u001b[39m, midi_name)\n\u001b[1;32m     16\u001b[0m midi_path_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(midi_path\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 18\u001b[0m pause_collector, nd, note_collector, note_collector_all,SAL,CD,time_passed_ar,creation_success \u001b[38;5;241m=\u001b[39m basic_info_extraction\u001b[38;5;241m.\u001b[39mbasic_tools_panda(file_name \u001b[38;5;241m=\u001b[39m midi_name, directory_path_with_tracks\u001b[38;5;241m=\u001b[39mmidi_path_path, start_time\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m),end_time\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# --> only note_collector of interest\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# change nan-values/-1 to 0:\u001b[39;00m\n\u001b[1;32m     22\u001b[0m note_collector[np\u001b[38;5;241m.\u001b[39mwhere(note_collector\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 8)"
     ]
    }
   ],
   "source": [
    "midi_path = '../../stage1_data_collecting_phase/audio2midi_converter/audio2midi_Wang/GeneratedMIDI_Wang/'\n",
    "midi_files = glob.glob(midi_path + '*.mid')\n",
    "row_len_list = []\n",
    "col_len_list = []\n",
    "\n",
    "symbolic_input_dict = dict()\n",
    "midi_name_list = [] # needed when extracting mel filterbank from .wav files. We don't want to do it for all\n",
    "# 404 files but only for the ones which are also available in midi.\n",
    "\n",
    "for midi_path in midi_files:\n",
    "    midi_name = midi_path.split('/')[-1].split('.')[0] # e.g. P_BeachBoys_GoodVib_accompaniment\n",
    "    \n",
    "    midi_name_list.append('_'.join(midi_name.split('_')[:-1]))\n",
    "    \n",
    "    print('current midi: ', midi_name)\n",
    "    midi_path_path = '/'.join(midi_path.split('/')[:-1]) +'/'\n",
    "    \n",
    "    pause_collector, nd, note_collector, note_collector_all,SAL,CD,time_passed_ar,creation_success = basic_info_extraction.basic_tools_panda(file_name = midi_name, directory_path_with_tracks=midi_path_path, start_time=float('inf'),end_time=float('-inf'))\n",
    "    # --> only note_collector of interest\n",
    "    \n",
    "    # change nan-values/-1 to 0:\n",
    "    note_collector[np.where(note_collector==-1)] = 0\n",
    "    \n",
    "    # filter out rows containing no notes:\n",
    "    ind_list = []\n",
    "    for ind,row in enumerate(note_collector):\n",
    "        if np.all(row==0) == False:\n",
    "            ind_list.append(ind)\n",
    "    row_mask = np.array(ind_list)\n",
    "    note_collector = note_collector[row_mask, :]\n",
    "    \n",
    "    # reduce note collector in size when it is too huge in column size:\n",
    "    aimed_col_length = 300\n",
    "    \n",
    "    if note_collector.shape[1] > aimed_col_length:\n",
    "\n",
    "        note_collector = reduce_symbolic_col_size(note_collector, final_length=aimed_col_length)\n",
    "    \n",
    "    \n",
    "    # collecting the size to determine the biggest note_collector later, such that every array will have\n",
    "    # same size:\n",
    "    row_len_list.append(note_collector.shape[0])\n",
    "    col_len_list.append(note_collector.shape[1])\n",
    "        \n",
    "    # normalize note values:\n",
    "    note_collector_normal =  (note_collector - note_collector.mean()) / note_collector.std()\n",
    "    \n",
    "    # to tensor:\n",
    "    note_collector_normal = torch.tensor(note_collector_normal)\n",
    "    \n",
    "    mp3_code = '_'.join(midi_name.split('_')[:-1]) # e.g. P_BeachBoys_GoodVib\n",
    "    if mp3_code in l1:\n",
    "        mp3_code = l2[l1.index(mp3_code)]\n",
    "    symbolic_input_dict[mp3_code] = note_collector_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "symbolic extraction done\n"
     ]
    }
   ],
   "source": [
    "# Find out the biggest size and zeropad files with lower shape[1], i. e. samples which are shorter in time \n",
    "# shall be expanded by 0-s:\n",
    "max_row = max(row_len_list)\n",
    "max_col = max(col_len_list)\n",
    "\n",
    "for key,value in  symbolic_input_dict.items():\n",
    "            \n",
    "    if value.shape[1] < max_col or value.shape[0] < max_row:\n",
    "        \n",
    "        value_tensor = value\n",
    "        \n",
    "        if value.shape[1] < max_col:\n",
    "            difference = max_col - value.shape[1]\n",
    "            to_add = torch.zeros((value_tensor.shape[0],difference))\n",
    "            value_tensor = torch.cat((value_tensor, to_add),axis=1)\n",
    "            \n",
    "        if value.shape[0] < max_row:\n",
    "            difference = max_row - value.shape[0]\n",
    "            to_add = torch.zeros((difference, value_tensor.shape[1]))\n",
    "            value_tensor =  torch.cat((value_tensor, to_add),axis=0)\n",
    "            \n",
    "        symbolic_input_dict[key] = value_tensor\n",
    "        \n",
    "# sort by key:\n",
    "symbolic_input_dict = dict(sorted(symbolic_input_dict.items()))\n",
    "\n",
    "print('symbolic extraction done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Transfer audio files to mel filtered STFT-s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mel_filterbank(sample_rate, hop_size, segment_times, STFT):\n",
    "\n",
    "    # from http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/#eqn1\n",
    "    # and https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html\n",
    "\n",
    "    STFT11 = np.copy(STFT)\n",
    "\n",
    "    # 0. finding out the sample rate for one window:\n",
    "    len_window = int(segment_times[1])\n",
    "\n",
    "    in_sec_window = (len_window-1) * hop_size/ sample_rate\n",
    "    sample_rate2 = int(in_sec_window * sample_rate) # that's the number of samples in one window\n",
    "    \n",
    "\n",
    "    # 1. lower border of human hearing https://www.audiologyresearch.org/human-hearing-range: \n",
    "    # 20 Hz:\n",
    "    # get borders mel band:\n",
    "    variable = 2959 # values seen in lecture and internet so far: 2959, 1125, 2595\n",
    "    \n",
    "    mel_lowest_f = (variable * np.log10(1 + (20) / 700))\n",
    "    mel_highest_f = (variable * np.log10(1 + (sample_rate2 / 2) / 700))  # Convert Hz to Mel\n",
    "\n",
    "    # 2. creating 40 bins/bands from which 38 are equally distributed between the lowest and\n",
    "    # highest mel value:\n",
    "    number_filters = 20 #40 # normal: 20-40, standard: 26\n",
    "    # with 20 no final_bins with same value and in filterbank no row with set={0.0}\n",
    "\n",
    "    distance_mel = (mel_lowest_f + mel_highest_f)/(number_filters+1)\n",
    "    mel_binDistances = np.array([mel_lowest_f + i * distance_mel for i in range(0, number_filters+2)])\n",
    "\n",
    "    # 3. Converting the the mel_bins into frequencies:\n",
    "    f_binDistances = (700 * (10**(mel_binDistances / variable) - 1)) #700*(np.exp(mel_binDistances/2959)-1)\n",
    "    # because log_10(b)=x leads to 10**x=b\n",
    "\n",
    "    # 4. Missing freq. resolution. Therefore, round the found freq. to nearest DFT bin.\n",
    "    # Need of number of DFT bins and sample_rate:\n",
    "\n",
    "    final_bins = np.floor((STFT.shape[0]+1)*f_binDistances/sample_rate2) # last bin relates\n",
    "    # to mel_highest_f\n",
    "\n",
    "    # 5. Create filterbanks: \n",
    "    # Each filterbank starts at point i, has the peak at i+1 and is zero at i+2.\n",
    "    # formula H_m(k) from http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/\n",
    "    # where k=...-th_filter, f=final_bins. Sentence before describes formula\n",
    "    filterbank = np.zeros((number_filters,int(np.floor(STFT.shape[0] ))))\n",
    "    # rows: number used filterbanks/individual filters, columns: values for certain frequencies\n",
    "\n",
    "    for m in range(1,(number_filters+1)):\n",
    "\n",
    "        left_val = int(final_bins[m-1])\n",
    "        middle_val = int(final_bins[m])\n",
    "        right_val = int(final_bins[m+1])\n",
    "\n",
    "        for k in range(left_val, middle_val): # number of fitlers m between final bins\n",
    "            filterbank[m-1,k] = (k-final_bins[m-1])/(final_bins[m]-final_bins[m-1])\n",
    "        for k in range(middle_val, right_val):\n",
    "            filterbank[m-1,k] = (final_bins[m+1]-k)/(final_bins[m+1]-final_bins[m])\n",
    "\n",
    "    # 6. lecture content L04 slide 54: the magnitude approach:\n",
    "    # Use the filterbank for each DFT magnitude to reduce it in bin size:\n",
    "    \n",
    "    shape_border = int(STFT.shape[1]/2)\n",
    "    S1 = (filterbank @ abs(STFT[:,:shape_border]))\n",
    "    S2 = (filterbank @ abs(STFT[:,:STFT.shape[1]]))\n",
    "    S = np.concatenate((S1,S2),axis=1)\n",
    "    S = 10 * np.log10(S+1) # rows: number of filters, column: like in STFT the \n",
    "\n",
    "\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def using_mel_filterbank(file, number_overlap = 100):    \n",
    "\n",
    "    sample_rate, data = scipy.io.wavfile.read(file)\n",
    "    try:\n",
    "        data = (data[:,0] + data[:,1])/2\n",
    "    except: # data has only one channel as we want it\n",
    "        pass\n",
    "\n",
    "    # with resampling the outcoming STFT tensor will have less columns:\n",
    "    #resampled = data\n",
    "    resampled = signal.resample(data, num=30000, t=None, axis=0, window=None)\n",
    "    \n",
    "    sample_freq, segment_times, STFT = signal.stft(resampled,noverlap=number_overlap) # all have the same size already!\n",
    "    # noverlap: the number of samples two neighbouring DFT have in common / reverse of hop size\n",
    "\n",
    "    hop_size = int(segment_times[1] - number_overlap)\n",
    "\n",
    "    mel = mel_filterbank(sample_rate, hop_size, segment_times, STFT)\n",
    "    # of course its the filtered STFT\n",
    "    \n",
    "    return mel, sample_rate, hop_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current file:  ../../stage1_data_collecting_phase/audio2midi_converter/track_preparation/accompaniment/P_BOBfeatBrunoMars_Nothing_accompaniment.wav\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'using_mel_filterbank' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcurrent file: \u001b[39m\u001b[38;5;124m'\u001b[39m, file)\n\u001b[1;32m     16\u001b[0m sample_rate, data \u001b[38;5;241m=\u001b[39m scipy\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mwavfile\u001b[38;5;241m.\u001b[39mread(file)\n\u001b[0;32m---> 18\u001b[0m mel, sample_rate, hop_size \u001b[38;5;241m=\u001b[39m \u001b[43musing_mel_filterbank\u001b[49m(file, number_overlap \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mp3_code \u001b[38;5;129;01min\u001b[39;00m l1:\n\u001b[1;32m     21\u001b[0m     mp3_code \u001b[38;5;241m=\u001b[39m l2[l1\u001b[38;5;241m.\u001b[39mindex(mp3_code)]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'using_mel_filterbank' is not defined"
     ]
    }
   ],
   "source": [
    "path =  '../../stage1_data_collecting_phase/audio2midi_converter/track_preparation/accompaniment/'\n",
    "\n",
    "filenames = glob.glob(path + \"*.wav\") # read all the files with extension .wav\n",
    "\n",
    "samples_mel_dict = dict() \n",
    "samples_mel_sizes = []\n",
    "\n",
    "for ind, file in enumerate(filenames):\n",
    "    \n",
    "    # Getting title of file:\n",
    "    mp3_code = (file.split('/')[-1]).replace('_accompaniment.wav','') \n",
    "    \n",
    "    if mp3_code in midi_name_list:\n",
    "        \n",
    "        print('current file: ', file)\n",
    "        sample_rate, data = scipy.io.wavfile.read(file)\n",
    "\n",
    "        mel, sample_rate, hop_size = using_mel_filterbank(file, number_overlap = 100)\n",
    "        \n",
    "        if mp3_code in l1:\n",
    "            mp3_code = l2[l1.index(mp3_code)]\n",
    "\n",
    "        # Typing result into dictionary:\n",
    "        samples_mel_dict[mp3_code] = torch.tensor(mel)\n",
    "        samples_mel_sizes.append(mel.shape[1])\n",
    "print('mel extraction done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out the biggest size and zeropad files with lower shape[1], i. e. samples which are shorter in time \n",
    "# shall be expanded by 0-s. So, we get for sure same batch sizes.:\n",
    "max_sample_len = max(samples_mel_sizes)\n",
    "\n",
    "for key,value in samples_mel_dict.items():\n",
    "    \n",
    "    # normalize mel:\n",
    "    value_normal = (value - value.mean()) / value.std()\n",
    "        \n",
    "    if value.shape[1] < max_sample_len:\n",
    "        difference = max_sample_len - value.shape[1]\n",
    "        to_add = torch.zeros((value.shape[0],difference))\n",
    "        samples_mel_dict[key] = torch.concatenate((value_normal, to_add),axis=1)\n",
    "    else:\n",
    "        samples_mel_dict[key] = value_normal\n",
    "        \n",
    "# sort by key:\n",
    "samples_mel_dict = dict(sorted(samples_mel_dict.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Scalar features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1. Import already built scalar dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>midi_Number of Pitches</th>\n",
       "      <th>midi_Number of Pitch Classes</th>\n",
       "      <th>midi_Number of Common Pitches</th>\n",
       "      <th>midi_Number of Common Pitch Classes</th>\n",
       "      <th>midi_Range</th>\n",
       "      <th>midi_Importance of Bass Register</th>\n",
       "      <th>midi_Importance of Middle Register</th>\n",
       "      <th>midi_Importance of High Register</th>\n",
       "      <th>midi_Dominant Spread</th>\n",
       "      <th>midi_Strong Tonal Centres</th>\n",
       "      <th>...</th>\n",
       "      <th>audio_W(239)_lld</th>\n",
       "      <th>audio_W(240)_lld</th>\n",
       "      <th>audio_W(241)_lld</th>\n",
       "      <th>audio_W(242)_lld</th>\n",
       "      <th>audio_W(243)_lld</th>\n",
       "      <th>audio_W(244)_lld</th>\n",
       "      <th>audio_W(245)_lld</th>\n",
       "      <th>audio_W(246)_lld</th>\n",
       "      <th>audio_W(247)_lld</th>\n",
       "      <th>audio_W(248)_lld</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>H_2Pac_AllEyez</th>\n",
       "      <td>-0.691050</td>\n",
       "      <td>0.006870</td>\n",
       "      <td>1.431812</td>\n",
       "      <td>-0.862500</td>\n",
       "      <td>-1.956030</td>\n",
       "      <td>0.173771</td>\n",
       "      <td>0.553268</td>\n",
       "      <td>-0.748694</td>\n",
       "      <td>-0.215021</td>\n",
       "      <td>1.896439</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.395981</td>\n",
       "      <td>-0.475631</td>\n",
       "      <td>-0.360202</td>\n",
       "      <td>-0.244258</td>\n",
       "      <td>-0.222566</td>\n",
       "      <td>3.228343</td>\n",
       "      <td>-0.538287</td>\n",
       "      <td>-1.170944</td>\n",
       "      <td>0.701196</td>\n",
       "      <td>-0.224492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_2Pac_KeepYaH</th>\n",
       "      <td>-0.992124</td>\n",
       "      <td>-0.546890</td>\n",
       "      <td>1.431812</td>\n",
       "      <td>0.567901</td>\n",
       "      <td>-1.644949</td>\n",
       "      <td>0.121261</td>\n",
       "      <td>0.606396</td>\n",
       "      <td>-0.748694</td>\n",
       "      <td>-0.215021</td>\n",
       "      <td>0.194290</td>\n",
       "      <td>...</td>\n",
       "      <td>0.308935</td>\n",
       "      <td>-0.475631</td>\n",
       "      <td>-0.360202</td>\n",
       "      <td>-1.270476</td>\n",
       "      <td>-0.222566</td>\n",
       "      <td>-0.328788</td>\n",
       "      <td>-0.538287</td>\n",
       "      <td>-1.170944</td>\n",
       "      <td>0.701196</td>\n",
       "      <td>-0.224492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_50Cent_CandySh</th>\n",
       "      <td>0.061634</td>\n",
       "      <td>0.006870</td>\n",
       "      <td>0.621390</td>\n",
       "      <td>-0.862500</td>\n",
       "      <td>-0.089542</td>\n",
       "      <td>1.697301</td>\n",
       "      <td>-1.752025</td>\n",
       "      <td>0.035989</td>\n",
       "      <td>-0.215021</td>\n",
       "      <td>0.194290</td>\n",
       "      <td>...</td>\n",
       "      <td>0.308935</td>\n",
       "      <td>0.384911</td>\n",
       "      <td>-0.360202</td>\n",
       "      <td>-0.244258</td>\n",
       "      <td>-0.222566</td>\n",
       "      <td>1.915512</td>\n",
       "      <td>-0.538287</td>\n",
       "      <td>0.421620</td>\n",
       "      <td>0.701196</td>\n",
       "      <td>-0.224492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_50Cent_InDaClu</th>\n",
       "      <td>0.513245</td>\n",
       "      <td>1.114391</td>\n",
       "      <td>0.621390</td>\n",
       "      <td>0.567901</td>\n",
       "      <td>0.377080</td>\n",
       "      <td>-0.007348</td>\n",
       "      <td>-0.834989</td>\n",
       "      <td>0.865110</td>\n",
       "      <td>-0.215021</td>\n",
       "      <td>0.194290</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.395981</td>\n",
       "      <td>-0.475631</td>\n",
       "      <td>2.653326</td>\n",
       "      <td>-0.244258</td>\n",
       "      <td>-0.222566</td>\n",
       "      <td>3.228343</td>\n",
       "      <td>-0.538287</td>\n",
       "      <td>-1.170944</td>\n",
       "      <td>0.701196</td>\n",
       "      <td>-0.224492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_ASAPRocky_Fashion</th>\n",
       "      <td>-1.142661</td>\n",
       "      <td>-1.100650</td>\n",
       "      <td>0.621390</td>\n",
       "      <td>0.567901</td>\n",
       "      <td>-1.800490</td>\n",
       "      <td>0.576342</td>\n",
       "      <td>0.145953</td>\n",
       "      <td>-0.748694</td>\n",
       "      <td>-0.949373</td>\n",
       "      <td>0.194290</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.395981</td>\n",
       "      <td>0.678700</td>\n",
       "      <td>2.653326</td>\n",
       "      <td>-0.244258</td>\n",
       "      <td>-0.222566</td>\n",
       "      <td>-0.328788</td>\n",
       "      <td>-0.538287</td>\n",
       "      <td>-1.170944</td>\n",
       "      <td>0.701196</td>\n",
       "      <td>-0.224492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3329 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     midi_Number of Pitches  midi_Number of Pitch Classes  \\\n",
       "name                                                                        \n",
       "H_2Pac_AllEyez                    -0.691050                      0.006870   \n",
       "H_2Pac_KeepYaH                    -0.992124                     -0.546890   \n",
       "H_50Cent_CandySh                   0.061634                      0.006870   \n",
       "H_50Cent_InDaClu                   0.513245                      1.114391   \n",
       "H_ASAPRocky_Fashion               -1.142661                     -1.100650   \n",
       "\n",
       "                     midi_Number of Common Pitches  \\\n",
       "name                                                 \n",
       "H_2Pac_AllEyez                            1.431812   \n",
       "H_2Pac_KeepYaH                            1.431812   \n",
       "H_50Cent_CandySh                          0.621390   \n",
       "H_50Cent_InDaClu                          0.621390   \n",
       "H_ASAPRocky_Fashion                       0.621390   \n",
       "\n",
       "                     midi_Number of Common Pitch Classes  midi_Range  \\\n",
       "name                                                                   \n",
       "H_2Pac_AllEyez                                 -0.862500   -1.956030   \n",
       "H_2Pac_KeepYaH                                  0.567901   -1.644949   \n",
       "H_50Cent_CandySh                               -0.862500   -0.089542   \n",
       "H_50Cent_InDaClu                                0.567901    0.377080   \n",
       "H_ASAPRocky_Fashion                             0.567901   -1.800490   \n",
       "\n",
       "                     midi_Importance of Bass Register  \\\n",
       "name                                                    \n",
       "H_2Pac_AllEyez                               0.173771   \n",
       "H_2Pac_KeepYaH                               0.121261   \n",
       "H_50Cent_CandySh                             1.697301   \n",
       "H_50Cent_InDaClu                            -0.007348   \n",
       "H_ASAPRocky_Fashion                          0.576342   \n",
       "\n",
       "                     midi_Importance of Middle Register  \\\n",
       "name                                                      \n",
       "H_2Pac_AllEyez                                 0.553268   \n",
       "H_2Pac_KeepYaH                                 0.606396   \n",
       "H_50Cent_CandySh                              -1.752025   \n",
       "H_50Cent_InDaClu                              -0.834989   \n",
       "H_ASAPRocky_Fashion                            0.145953   \n",
       "\n",
       "                     midi_Importance of High Register  midi_Dominant Spread  \\\n",
       "name                                                                          \n",
       "H_2Pac_AllEyez                              -0.748694             -0.215021   \n",
       "H_2Pac_KeepYaH                              -0.748694             -0.215021   \n",
       "H_50Cent_CandySh                             0.035989             -0.215021   \n",
       "H_50Cent_InDaClu                             0.865110             -0.215021   \n",
       "H_ASAPRocky_Fashion                         -0.748694             -0.949373   \n",
       "\n",
       "                     midi_Strong Tonal Centres  ...  audio_W(239)_lld  \\\n",
       "name                                            ...                     \n",
       "H_2Pac_AllEyez                        1.896439  ...         -0.395981   \n",
       "H_2Pac_KeepYaH                        0.194290  ...          0.308935   \n",
       "H_50Cent_CandySh                      0.194290  ...          0.308935   \n",
       "H_50Cent_InDaClu                      0.194290  ...         -0.395981   \n",
       "H_ASAPRocky_Fashion                   0.194290  ...         -0.395981   \n",
       "\n",
       "                     audio_W(240)_lld  audio_W(241)_lld  audio_W(242)_lld  \\\n",
       "name                                                                        \n",
       "H_2Pac_AllEyez              -0.475631         -0.360202         -0.244258   \n",
       "H_2Pac_KeepYaH              -0.475631         -0.360202         -1.270476   \n",
       "H_50Cent_CandySh             0.384911         -0.360202         -0.244258   \n",
       "H_50Cent_InDaClu            -0.475631          2.653326         -0.244258   \n",
       "H_ASAPRocky_Fashion          0.678700          2.653326         -0.244258   \n",
       "\n",
       "                     audio_W(243)_lld  audio_W(244)_lld  audio_W(245)_lld  \\\n",
       "name                                                                        \n",
       "H_2Pac_AllEyez              -0.222566          3.228343         -0.538287   \n",
       "H_2Pac_KeepYaH              -0.222566         -0.328788         -0.538287   \n",
       "H_50Cent_CandySh            -0.222566          1.915512         -0.538287   \n",
       "H_50Cent_InDaClu            -0.222566          3.228343         -0.538287   \n",
       "H_ASAPRocky_Fashion         -0.222566         -0.328788         -0.538287   \n",
       "\n",
       "                     audio_W(246)_lld  audio_W(247)_lld  audio_W(248)_lld  \n",
       "name                                                                       \n",
       "H_2Pac_AllEyez              -1.170944          0.701196         -0.224492  \n",
       "H_2Pac_KeepYaH              -1.170944          0.701196         -0.224492  \n",
       "H_50Cent_CandySh             0.421620          0.701196         -0.224492  \n",
       "H_50Cent_InDaClu            -1.170944          0.701196         -0.224492  \n",
       "H_ASAPRocky_Fashion         -1.170944          0.701196         -0.224492  \n",
       "\n",
       "[5 rows x 3329 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We don't care whether categorical or regressive dataset as the labels gets dropped:\n",
    "scalar_df = pd.read_csv('../unimodal_scalarFeatures_model/dataframes/audioAndSymbolic_dataframe_categorical_audio2midiWang.csv')\n",
    "\n",
    "# Change names to MP3 code:\n",
    "if '_accompaniment' in scalar_df['name'][0]: \n",
    "    name_list_scalar = ['_'.join(i.split('_')[:-1])[1:] for i in list(scalar_df['name'])]\n",
    "    scalar_df['name'] = name_list_scalar\n",
    "      \n",
    "scalar_df.set_index('name', inplace=True)\n",
    "\n",
    "# drop label column:\n",
    "scalar_df.drop(['final_label'], axis=1, inplace=True) \n",
    "scalar_df.sort_index(inplace=True)\n",
    "\n",
    "scalar_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2. Keep only top k features of scalar_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start finding top k scalar features...\n",
      "Do you want to use the already provided top 30 features?[y/n]y\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'scalar_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDo you want to use the already provided top 30 features?[y/n]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m user_input\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m----> 5\u001b[0m     scalar_df \u001b[38;5;241m=\u001b[39m \u001b[43mscalar_df\u001b[49m[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio_tonal.hpcp_entropy.stdev\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio_pcm_fftMag_spectralRollOff90.0_sma_leftctime_f2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio_audspec_lengthL1norm_sma_de_stddevRisingSlope_f2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio_F0final_sma_de_flatness_f2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio_pcm_fftMag_spectralSlope_sma_de_maxSegLen_f2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio_logHNR_sma_de_lpgain_f2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio_jitterDDP_sma_linregc2_f2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio_pcm_fftMag_spectralSkewness_sma_peakDistStddev_f2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio_pcm_fftMag_spectralEntropy_sma_de_iqr2-3_f2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio_lowlevel.dissonance.mean\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio_F0_sma_maxPos\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio_logHNR_sma_lpc4_f2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio_tonal.hpcp_entropy.mean\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio_pcm_fftMag_spectralVariance_sma_de_iqr2-3_f2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio_audspec_lengthL1norm_sma_lpc3_f2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio_pcm_fftMag_spectralSlope_sma_de_quartile2_f2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio_pcm_fftMag_spectralSkewness_sma_lpc2_f2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio_pcm_fftMag_spectralSlope_sma_de_minRangeRel_f2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio_pcm_fftMag_spectralEntropy_sma_de_quartile3_f2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio_audspec_lengthL1norm_sma_de_iqr2-3_f2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio_pcm_fftMag_spectralEntropy_sma_de_iqr1-2_f2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio_shimmerLocal_sma_de_upleveltime90_f2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio_tonal.tuning_diatonic_strength\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio_lowlevel.spectral_entropy.mean\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio_audspec_lengthL1norm_sma_de_upleveltime50_f2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio_audspec_lengthL1norm_sma_iqr2-3_f2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio_audspec_lengthL1norm_sma_de_iqr1-3_f2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmidi_Average Note Duration\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio_audspec_lengthL1norm_sma_de_quartile1_f2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio_audspec_lengthL1norm_sma_de_iqr1-2_f2\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m     sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../unimodal_scalarFeatures_model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scalar_df' is not defined"
     ]
    }
   ],
   "source": [
    "print('start finding top k scalar features...')\n",
    "user_input = input('Do you want to use the already provided top 30 features?[y/n]')\n",
    "\n",
    "if user_input.lower() == 'y':\n",
    "    scalar_df = scalar_df[['audio_tonal.hpcp_entropy.stdev', 'audio_pcm_fftMag_spectralRollOff90.0_sma_leftctime_f2', 'audio_audspec_lengthL1norm_sma_de_stddevRisingSlope_f2', 'audio_F0final_sma_de_flatness_f2', 'audio_pcm_fftMag_spectralSlope_sma_de_maxSegLen_f2', 'audio_logHNR_sma_de_lpgain_f2', 'audio_jitterDDP_sma_linregc2_f2', 'audio_pcm_fftMag_spectralSkewness_sma_peakDistStddev_f2', 'audio_pcm_fftMag_spectralEntropy_sma_de_iqr2-3_f2', 'audio_lowlevel.dissonance.mean', 'audio_F0_sma_maxPos', 'audio_logHNR_sma_lpc4_f2', 'audio_tonal.hpcp_entropy.mean', 'audio_pcm_fftMag_spectralVariance_sma_de_iqr2-3_f2', 'audio_audspec_lengthL1norm_sma_lpc3_f2', 'audio_pcm_fftMag_spectralSlope_sma_de_quartile2_f2', 'audio_pcm_fftMag_spectralSkewness_sma_lpc2_f2', 'audio_pcm_fftMag_spectralSlope_sma_de_minRangeRel_f2', 'audio_pcm_fftMag_spectralEntropy_sma_de_quartile3_f2', 'audio_audspec_lengthL1norm_sma_de_iqr2-3_f2', 'audio_pcm_fftMag_spectralEntropy_sma_de_iqr1-2_f2', 'audio_shimmerLocal_sma_de_upleveltime90_f2', 'audio_tonal.tuning_diatonic_strength', 'audio_lowlevel.spectral_entropy.mean', 'audio_audspec_lengthL1norm_sma_de_upleveltime50_f2', 'audio_audspec_lengthL1norm_sma_iqr2-3_f2', 'audio_audspec_lengthL1norm_sma_de_iqr1-3_f2', 'midi_Average Note Duration', 'audio_audspec_lengthL1norm_sma_de_quartile1_f2', 'audio_audspec_lengthL1norm_sma_de_iqr1-2_f2']]\n",
    "    \n",
    "else:\n",
    "    sys.path.insert(0, '../unimodal_scalarFeatures_model')\n",
    "    import feature_reduction as fr\n",
    "\n",
    "    instance = fr.PipelinedScalarTraining()\n",
    "    instance.x = scalar_df\n",
    "\n",
    "    mask_for_reg_labels_top = regres_labels.index.isin(scalar_df.index) \n",
    "    regres_labels_top = regres_labels[mask_for_reg_labels_top]\n",
    "    regres_labels_top[['y_sublim','y_ease','y_vital']] = pd.DataFrame(regres_labels_top.final_label.tolist(), index=regres_labels_top.index)\n",
    "    del regres_labels_top['final_label']\n",
    "\n",
    "    instance.y = np.array(regres_labels_top)\n",
    "    instance.select_top_features(feature_number_approach1=20, feature_number_approach2=20, categorical=False)\n",
    "    scalar_df = instance.x\n",
    "    scalar_df.shape\n",
    "\n",
    "print('...done finding top k scalar features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Prepare inputs for model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.1. Bring dataset parts together. Remove samples which are not represented in all dataset parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask the other feature inputs of the same dataset:\n",
    "# for mel filterbank samples:\n",
    "mask_for_scalar_df = scalar_df.index.isin(symbolic_input_dict) & scalar_df.index.isin(samples_mel_dict) & scalar_df.index.isin(regres_labels.index) & scalar_df.index.isin(categorical_labels.index)\n",
    "mask_for_cat_labels = categorical_labels.index.isin(symbolic_input_dict) & categorical_labels.index.isin(samples_mel_dict) & categorical_labels.index.isin(scalar_df.index) & categorical_labels.index.isin(regres_labels.index)\n",
    "mask_for_reg_labels = regres_labels.index.isin(symbolic_input_dict) & regres_labels.index.isin(samples_mel_dict) & regres_labels.index.isin(scalar_df.index) & regres_labels.index.isin(categorical_labels.index)\n",
    "\n",
    "\n",
    "# convert dictionary keys to df to filter them. Then use them as mask for the dictionary:\n",
    "mel_dict = pd.DataFrame(columns=['name'])\n",
    "mel_dict['name'] = list(samples_mel_dict.keys())\n",
    "mel_dict.set_index('name', inplace=True)\n",
    "\n",
    "mask_for_mel_dict = mel_dict.index.isin(symbolic_input_dict) & mel_dict.index.isin(regres_labels.index) & mel_dict.index.isin(scalar_df.index) & mel_dict.index.isin(categorical_labels.index)\n",
    "mel_filter = mel_dict[mask_for_mel_dict]\n",
    "\n",
    "samples_mel_list = list(map(samples_mel_dict.get, mel_filter.index))\n",
    "\n",
    "# use the masks for the dataframes:\n",
    "scalar_df = scalar_df[mask_for_scalar_df]\n",
    "categorical_labels = categorical_labels[mask_for_cat_labels]\n",
    "regres_labels = regres_labels[mask_for_reg_labels]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.2. Convert dataset parts to usable inputs of the multimodal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for symbolic note array samples:\n",
    "# convert dictionary keys to df to filter them. Then use them as mask for the dictionary:\n",
    "note_dict = pd.DataFrame(columns=['name'])\n",
    "note_dict['name'] = list(symbolic_input_dict.keys())\n",
    "note_dict.set_index('name', inplace=True)\n",
    "\n",
    "mask_for_note_dict = note_dict.index.isin(samples_mel_dict) & note_dict.index.isin(regres_labels.index) & note_dict.index.isin(scalar_df.index) & note_dict.index.isin(categorical_labels.index)\n",
    "note_filter = note_dict[mask_for_note_dict]\n",
    "\n",
    "symbolic_input_list = list(map(symbolic_input_dict.get, note_filter.index))\n",
    "\n",
    "# stack all single note_arrays in symbolic_input_list together such that each tensor has same shape:\n",
    "max_width, max_height = 0,0\n",
    "for ele in symbolic_input_list:\n",
    "    width = ele.shape[1]\n",
    "    height = ele.shape[0]\n",
    "    \n",
    "    if max_width==0:\n",
    "        max_width = width\n",
    "        max_height = height\n",
    "        \n",
    "    elif max_width < width:\n",
    "        max_width = width\n",
    "        \n",
    "    elif max_height < height:\n",
    "        max_height = height\n",
    "\n",
    "tensor_list = []\n",
    "for sample_tensor in symbolic_input_list:\n",
    "    ground_tensor = torch.zeros((max_height, max_width))\n",
    "    ground_tensor[:sample_tensor.shape[0], :sample_tensor.shape[1]] = sample_tensor\n",
    "    tensor_list.append(ground_tensor)\n",
    "    \n",
    "symbolic_input_tensor = torch.stack(tensor_list, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert the dataframes:\n",
    "scalar_tensor = torch.tensor(scalar_df.values, dtype=torch.float32) # float32 because the network \n",
    "# weights are also defined with that data type\n",
    "\n",
    "\n",
    "# split the list label entities into several columns:\n",
    "categorical_labels[['y_sublim','y_ease','y_vital']] = pd.DataFrame(categorical_labels.final_label.tolist(), index=categorical_labels.index)\n",
    "categorical_labels.drop('final_label', inplace=True, axis=1)\n",
    "\n",
    "categorical_labels_tensor = torch.tensor(categorical_labels.values, dtype=torch.int32)\n",
    "\n",
    "\n",
    "regres_labels[['y_sublim','y_ease','y_vital']] = pd.DataFrame(regres_labels.final_label.tolist(), index=regres_labels.index)\n",
    "regres_labels.drop('final_label', inplace=True, axis=1)\n",
    "\n",
    "regres_labels_tensor = torch.tensor(regres_labels.values, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# convert lists of tensors into 3D tensor:\n",
    "samples_mel_tensor = torch.stack([sample_tensor for sample_tensor in samples_mel_list], dim=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Generate inputs based on MIDI files for the case that the MusicBERT model is used instead of the note array as symbolic track of the multi-modal model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load musicBERT model with all its pre-trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disable_cp = False\n",
      "mask_strategy = ['bar']\n",
      "convert_encoding = OCTMIDI\n",
      "crop_length = None\n",
      "Your version identical with original version \n",
      "-> fairseq \n",
      "-> 0.10.2\n",
      "\n",
      "\n",
      "For \n",
      "ast \n",
      "python version needs to fit \n",
      "-> original version 3.7.3 \n",
      "-> your version 3.8.10 (default, Jun  4 2021, 15:09:15) \n",
      "[GCC 7.5.0]\n",
      "\n",
      "\n",
      "For \n",
      "collections \n",
      "python version needs to fit \n",
      "-> original version 3.7.3 \n",
      "-> your version 3.8.10 (default, Jun  4 2021, 15:09:15) \n",
      "[GCC 7.5.0]\n",
      "\n",
      "\n",
      "For \n",
      "contextlib \n",
      "python version needs to fit \n",
      "-> original version 3.7.3 \n",
      "-> your version 3.8.10 (default, Jun  4 2021, 15:09:15) \n",
      "[GCC 7.5.0]\n",
      "\n",
      "\n",
      "For \n",
      "inspect \n",
      "python version needs to fit \n",
      "-> original version 3.7.3 \n",
      "-> your version 3.8.10 (default, Jun  4 2021, 15:09:15) \n",
      "[GCC 7.5.0]\n",
      "\n",
      "\n",
      "For \n",
      "logging \n",
      "python version needs to fit \n",
      "-> original version 3.7.3 \n",
      "-> your version 3.8.10 (default, Jun  4 2021, 15:09:15) \n",
      "[GCC 7.5.0]\n",
      "\n",
      "\n",
      "For \n",
      "os \n",
      "python version needs to fit \n",
      "-> original version 3.7.3 \n",
      "-> your version 3.8.10 (default, Jun  4 2021, 15:09:15) \n",
      "[GCC 7.5.0]\n",
      "\n",
      "\n",
      "Your version identical with original version \n",
      "-> re \n",
      "-> 2.2.1\n",
      "\n",
      "\n",
      "Possibly different versions: \n",
      "-> time \n",
      "-> original version 3.1.2 \n",
      "-> your version  is not clearly visible. Go to your used python folder path .../python3.7/site-packages to investigate the version of your package. Also possible: have a look at the shell commands \"pip show module_name\" and \"apt show module_name\"\n",
      "\n",
      "\n",
      "For \n",
      "traceback \n",
      "python version needs to fit \n",
      "-> original version 3.7.3 \n",
      "-> your version 3.8.10 (default, Jun  4 2021, 15:09:15) \n",
      "[GCC 7.5.0]\n",
      "\n",
      "\n",
      "For \n",
      "pathlib \n",
      "python version needs to fit \n",
      "-> original version 3.7.3 \n",
      "-> your version 3.8.10 (default, Jun  4 2021, 15:09:15) \n",
      "[GCC 7.5.0]\n",
      "\n",
      "\n",
      "For \n",
      "urllib \n",
      "python version needs to fit \n",
      "-> original version 3.7.3 \n",
      "-> your version 3.8.10 (default, Jun  4 2021, 15:09:15) \n",
      "[GCC 7.5.0]\n",
      "\n",
      "\n",
      "Possibly different versions: \n",
      "-> sys \n",
      "-> original version 3.7.3 \n",
      "-> your version 3.8.10 (default, Jun  4 2021, 15:09:15) \n",
      "[GCC 7.5.0]\n",
      "\n",
      "\n",
      "loading model and data\n",
      "my model path MIDIBert\n",
      "/home/c/Schreibtisch/ba_08102022/stage3_models/multimodal_model/MIDIBert\n"
     ]
    }
   ],
   "source": [
    "import MIDIBert.my_musicBERT_loader as berter\n",
    "music_bert_model = berter.load()\n",
    "music_bert_model = music_bert_model.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your version identical with original version \n",
      "-> music21 \n",
      "-> 7.3.3\n",
      "\n",
      "\n",
      "Your version identical with original version \n",
      "-> miditoolkit \n",
      "-> 0.1.16\n",
      "\n",
      "\n",
      "For \n",
      "random \n",
      "python version needs to fit \n",
      "-> original version 3.7.3 \n",
      "-> your version 3.8.10 (default, Jun  4 2021, 15:09:15) \n",
      "[GCC 7.5.0]\n",
      "\n",
      "\n",
      "Possibly different versions: \n",
      "-> time \n",
      "-> original version 3.1.2 \n",
      "-> your version  is not clearly visible. Go to your used python folder path .../python3.7/site-packages to investigate the version of your package. Also possible: have a look at the shell commands \"pip show module_name\" and \"apt show module_name\"\n",
      "\n",
      "\n",
      "For \n",
      "math \n",
      "python version needs to fit \n",
      "-> original version 3.7.3 \n",
      "-> your version 3.8.10 (default, Jun  4 2021, 15:09:15) \n",
      "[GCC 7.5.0]\n",
      "\n",
      "\n",
      "Possibly different versions: \n",
      "-> sys \n",
      "-> original version 3.7.3 \n",
      "-> your version 3.8.10 (default, Jun  4 2021, 15:09:15) \n",
      "[GCC 7.5.0]\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'note_filter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mMIDIBert\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmusicBERT_predictions\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mbert_predictor\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m filtered_mp3code_list \u001b[38;5;241m=\u001b[39m (\u001b[43mnote_filter\u001b[49m\u001b[38;5;241m.\u001b[39mindex)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m      3\u001b[0m midibert_predictions \u001b[38;5;241m=\u001b[39m bert_predictor\u001b[38;5;241m.\u001b[39mpredictor(filtered_mp3code_list, music_bert_model, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m, path_to_midi \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../stage1_data_collecting_phase/audio2midi_converter/audio2midi_Wang/GeneratedMIDI_Wang\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'note_filter' is not defined"
     ]
    }
   ],
   "source": [
    "import MIDIBert.musicBERT_predictions as bert_predictor\n",
    "filtered_mp3code_list = (note_filter.index).tolist()\n",
    "midibert_predictions = bert_predictor.predictor(filtered_mp3code_list, music_bert_model, device='cpu', path_to_midi ='../../stage1_data_collecting_phase/audio2midi_converter/audio2midi_Wang/GeneratedMIDI_Wang')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([360, 20, 291]),\n",
       " torch.Size([360, 1, 300]),\n",
       " torch.Size([360, 30]),\n",
       " torch.Size([360, 3]),\n",
       " torch.Size([360, 3]),\n",
       " torch.Size([360, 13]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_mel_tensor.shape, symbolic_input_tensor.shape, scalar_tensor.shape, categorical_labels_tensor.shape, regres_labels_tensor.shape, midibert_predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.3. Store tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "input_folder = 'created_input_tensors'\n",
    "os.makedirs(input_folder, exist_ok=True)\n",
    "\n",
    "torch.save(samples_mel_tensor, input_folder+'/mel_tensor.pt')\n",
    "torch.save(symbolic_input_tensor, input_folder+'/symbolic_input_tensor.pt')\n",
    "torch.save(scalar_tensor, input_folder+'/scalar_tensor.pt')\n",
    "\n",
    "torch.save(categorical_labels_tensor, input_folder+'/categorical_labels_tensor.pt')\n",
    "torch.save(regres_labels_tensor, input_folder+'/regressive_labels_tensor.pt')\n",
    "\n",
    "torch.save(midibert_predictions, input_folder+'/midibert_input_tensor.pt')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Neural Network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Build up LSTM boxes for audio part - mel filtered STFT AND symbolic part can also use the same LSTM architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedBiLSTM(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_feat, hidden_dim, out_dim):\n",
    "        # row_col_id for symbolic input note_collector meaning: all notes played at the same time neighbourhood\n",
    "        # (more or less, note_collector columns aren't exactly building up times, i.e. first note of voice1\n",
    "        # is in the same column as first note of voice2). \n",
    "        # row_col_id for audio input mel filtered STFT meaning: we have fixed time windows from which each one\n",
    "        # will be one input.\n",
    "        # ROWS ARE MEANT OF INPUT\n",
    "        super().__init__() # super(NextCharLSTM, self).__init__()\n",
    "        \n",
    "        #self.embedding = nn.Embedding(row_col_id, embedding_dim) # size dict embeddings; size each \n",
    "        # embedding vector\n",
    "        self.lstm = nn.LSTM(input_size=num_feat, hidden_size=hidden_dim, bidirectional=True, \n",
    "                            num_layers=3, dropout=0.2, batch_first=True,\n",
    "                            proj_size=out_dim) # number features; number features in hidden state\n",
    "        # bidirectional LSTM and dropout=0.2 worked in m_c.pdf the best\n",
    "        # num_layers mean how many LSTM to stack\n",
    "        # batch_first=True gives (batchsize, seq_len, out_len) as output instead of (seq_len, batchsize, out_len)\n",
    "        # proj_size = when 0 (default) then output has hidden size, hidden_size should be by nature of size of output\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "        #self.linear = nn.Linear(hidden_dim, alphabet_size) # ???? change size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # change x shape from (batch_size x rows/features x columns/sequence)\n",
    "        # to (batch_size x columns/sequence x rows/features) which is the input of nn.LSTM\n",
    "        x = torch.swapdims(x, 1, 2)\n",
    "        \n",
    "        #x = self.embedding(x) \n",
    "        output, (final_hidden_state, final_cell_state) = self.lstm(x)\n",
    "        #logits = self.linear(final_hidden_state) # when arguemt x has an shape of \n",
    "        # [sample_character_string, int_encoding] the logits have output  \n",
    "        # [sample_character_string, int_encoding, alphabet_size] \n",
    "        \n",
    "        return output#final_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Build MLP which will be integrated in the multimodal net used for the scalar features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScalarMLP(nn.Module):\n",
    "    def __init__(self, num_feat: int, hidden_dim, out_dim):\n",
    "        super().__init__() \n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(in_features=num_feat, out_features=hidden_dim, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_dim, out_features=hidden_dim, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_dim, out_features=out_dim, bias=True),\n",
    "            nn.ReLU()\n",
    "            \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Input the scalar features into the net:\\nscalar_net = ScalarMLP(scalar_df.shape[1])\\nscalar_tensor = torch.tensor(scalar_df.values, dtype=torch.float32)\\nscalar_hidden_out = scalar_net.forward(scalar_tensor)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Input the scalar features into the net:\n",
    "scalar_net = ScalarMLP(scalar_df.shape[1])\n",
    "scalar_tensor = torch.tensor(scalar_df.values, dtype=torch.float32)\n",
    "scalar_hidden_out = scalar_net.forward(scalar_tensor)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Overall model which connects the different inputs with each other by integrating the individual neural models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model architecture idea comes from the paper 'MULTI-MODAL EMOTION RECOGNITION ON IEMOCAP WITH NEURAL NETWORKS' (Tripathi et al., n.d., https://arxiv.org/abs/1804.05788)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullModel(nn.Module): \n",
    "    \n",
    "    def __init__(self,conv_mode=True,\n",
    "                 num_feat_stft=-1, time_stft=-1, hidden_dim_stft=256, out_dim_stft=0, bi_direct_stft=True,\n",
    "                 kernel_size_stft=(5,5), channel_size_stft=6,\n",
    "                 midibert_mode=True, num_feat_note=-1, time_note=-1, hidden_dim_note=256, out_dim_note=0, bi_direct_note=True,\n",
    "                 dense_out_stft=256, \n",
    "                 dense_out_note=13,#256,\n",
    "                 final_hidden_dense_out=256,\n",
    "                 num_feat_scalar:int =-1, hidden_dim_scalar=-1, out_dim_scalar=-1,\n",
    "                 dense_out_scalar=256,\n",
    "                 device='cpu',\n",
    "                 label_type_categorical=False):\n",
    "                 # None/-1 when we put in values later\n",
    "        \n",
    "        super().__init__() \n",
    "        self.midibert_mode = midibert_mode\n",
    "        self.conv_mode = conv_mode\n",
    "        self.label_type_categorical = label_type_categorical\n",
    "        \n",
    "        # compute number of remaining height (features) after each convolution\n",
    "        if conv_mode and num_feat_stft % 2 != 0:\n",
    "            num_feat_stft += 1\n",
    "            \n",
    "        if conv_mode and time_stft % 2 != 0:  \n",
    "            time_stft += 1\n",
    "            \n",
    "        conv_out_comp = lambda x,k: int(((x - (k-1)*2 -1 + 2*0)/1 +1)/2)\n",
    "        \n",
    "        \n",
    "        # height audio input:\n",
    "        stage1_h = conv_out_comp(num_feat_stft, kernel_size_stft[0])\n",
    "        \n",
    "        if stage1_h % 2 != 0:\n",
    "            pool_stage1_h = 1\n",
    "        else:\n",
    "            pool_stage1_h = 0\n",
    "        \n",
    "        stage2_h = conv_out_comp(stage1_h+pool_stage1_h, kernel_size_stft[0])\n",
    "       \n",
    "        \n",
    "        # width audio input:\n",
    "        stage1_w = conv_out_comp(time_stft, kernel_size_stft[1])\n",
    "        \n",
    "        if stage1_w % 2 != 0:\n",
    "            pool_stage1_w = 1\n",
    "        else:\n",
    "            pool_stage1_w = 0\n",
    "        \n",
    "        stage2_w = conv_out_comp(stage1_w+pool_stage1_w, kernel_size_stft[1])\n",
    "\n",
    "\n",
    "        self.stft_block = nn.Sequential(\n",
    "            \n",
    "        nn.Conv2d(in_channels=1, out_channels=channel_size_stft, kernel_size=(kernel_size_stft[0],kernel_size_stft[1]), stride=1, padding=0, dilation=2, groups=1, bias=True, padding_mode='zeros'),\n",
    "        nn.MaxPool2d((2,2)),\n",
    "        nn.BatchNorm2d(channel_size_stft, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "        nn.ZeroPad2d((pool_stage1_w, 0, pool_stage1_h, 0)), # left, right, top, bottom\n",
    "\n",
    "        nn.Conv2d(in_channels=channel_size_stft, out_channels=channel_size_stft, kernel_size=(kernel_size_stft[0],kernel_size_stft[1]), stride=1, padding=0, dilation=2, groups=1, bias=True, padding_mode='zeros'),\n",
    "        nn.MaxPool2d((2,2)),\n",
    "        nn.BatchNorm2d(channel_size_stft, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "             \n",
    "        nn.Flatten(start_dim=1, end_dim=2), # assume batched input then [32,1,5,5]-->[32,5,5]\n",
    "            \n",
    "        StackedBiLSTM(stage2_h*channel_size_stft, hidden_dim_stft, out_dim_stft)\n",
    "        ).to(device)\n",
    "        \n",
    "        self.stft_lstm = StackedBiLSTM(num_feat_stft, hidden_dim_stft, out_dim_stft).to(device)\n",
    "        \n",
    "        if bi_direct_stft == True and out_dim_stft>0:\n",
    "            in_feat_stft = out_dim_stft*2\n",
    "        elif bi_direct_stft == True and out_dim_stft==0:\n",
    "            in_feat_stft = hidden_dim_stft*2\n",
    "        elif bi_direct_stft == True and out_dim_stft>0:\n",
    "            in_feat_stft = out_dim_stft\n",
    "        elif bi_direct_stft == False and out_dim_stft==0:\n",
    "            in_feat_stft = hidden_dim_stft\n",
    "        \n",
    "        if conv_mode:\n",
    "            time_stft = stage2_w\n",
    "            \n",
    "        self.stft_dense_layer = nn.Sequential(\n",
    "            nn.Flatten(start_dim=1, end_dim=- 1),\n",
    "            nn.Linear(in_features=in_feat_stft*time_stft, out_features=dense_out_stft, bias=True),\n",
    "            nn.ReLU()).to(device)\n",
    "\n",
    "        \n",
    "        \n",
    "        self.note_lstm = StackedBiLSTM(num_feat_note, hidden_dim_note, out_dim_note).to(device)\n",
    "        \n",
    "        if bi_direct_note == True and out_dim_note>0:\n",
    "            in_feat_note = out_dim_note*2\n",
    "        elif bi_direct_note == True and out_dim_note==0:\n",
    "            in_feat_note = hidden_dim_note*2\n",
    "        elif bi_direct_note == True and out_dim_note>0:\n",
    "            in_feat_note = out_dim_note\n",
    "        elif bi_direct_note == False and out_dim_note==0:\n",
    "            in_feat_note = hidden_dim_note\n",
    "        \n",
    "        self.note_dense_layer = nn.Sequential(\n",
    "            nn.Flatten(start_dim=1, end_dim=- 1),\n",
    "            nn.Linear(in_features=in_feat_note*time_note, out_features=dense_out_note, bias=True),\n",
    "            nn.ReLU()).to(device)\n",
    "\n",
    "        \n",
    "        self.scalar_mlp = ScalarMLP(num_feat_scalar, hidden_dim_scalar, out_dim_scalar).to(device)\n",
    "        \n",
    "        self.scalar_dense_layer = nn.Sequential(\n",
    "            nn.Linear(in_features=out_dim_scalar, out_features=dense_out_scalar, bias=True),\n",
    "            nn.ReLU()).to(device)\n",
    "            \n",
    "        \n",
    "        self.final_part = nn.Sequential(\n",
    "            nn.Linear(in_features=dense_out_stft+dense_out_note+dense_out_scalar, out_features=final_hidden_dense_out, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=final_hidden_dense_out, out_features=3, bias=True) # 3 labels (GEMS factors to predict)\n",
    "        ).to(device)\n",
    "        \n",
    "        self.device = device\n",
    "        self.label_type_categorical = label_type_categorical\n",
    "        if label_type_categorical: # number outputs for categorical case\n",
    "            out_feat = 2\n",
    "        else: # number of outputs for regressive case\n",
    "            out_feat = 1\n",
    "        \n",
    "        self.dim1 = nn.Sequential(\n",
    "            nn.Linear(in_features=dense_out_stft+dense_out_note+dense_out_scalar, out_features=final_hidden_dense_out, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=final_hidden_dense_out, out_features=out_feat, bias=True)\n",
    "        ).to(device)\n",
    "        \n",
    "        self.dim2 = nn.Sequential(\n",
    "            nn.Linear(in_features=dense_out_stft+dense_out_note+dense_out_scalar, out_features=final_hidden_dense_out, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=final_hidden_dense_out, out_features=out_feat, bias=True)\n",
    "        ).to(device)\n",
    "            \n",
    "        self.dim3 = nn.Sequential(\n",
    "            nn.Linear(in_features=dense_out_stft+dense_out_note+dense_out_scalar, out_features=final_hidden_dense_out, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=final_hidden_dense_out, out_features=out_feat, bias=True)\n",
    "        ).to(device)\n",
    "        \n",
    "        self.regressive_tanh = nn.Tanh().to(device)              \n",
    "        \n",
    "    def forward(self, x_stft, x_symbolic_note_ar, x_scalar):\n",
    "        \n",
    "        if self.conv_mode:\n",
    "            # when input is odd for convolutional layers:\n",
    "            padding = True\n",
    "            # shape of x_stft is batchxfeaturextime\n",
    "            if x_stft.shape[1] % 2 != 0 and x_stft.shape[2] % 2 != 0:\n",
    "                padder = nn.ZeroPad2d((1, 0, 1, 0)) # left, right, top, bottom\n",
    "            elif x_stft.shape[1] % 2 != 0:\n",
    "                 padder = nn.ZeroPad2d((0, 0, 1, 0))\n",
    "            elif x_stft.shape[2] % 2 != 0:\n",
    "                padder = nn.ZeroPad2d((1, 0, 0, 0))\n",
    "            else:\n",
    "                padding = False\n",
    "\n",
    "            if padding:\n",
    "                x_stft = padder(x_stft)\n",
    "                \n",
    "            # add channel:\n",
    "            x_stft = x_stft.unsqueeze(dim=1) # [4,20,290]-->[4,1,20,290]\n",
    "\n",
    "            h1_stft = self.stft_block(x_stft.to(torch.float32).to(self.device))\n",
    "        else:                       \n",
    "            h1_stft = self.stft_lstm(x_stft.to(torch.float32).to(self.device))\n",
    "                                  \n",
    "        h2_stft = self.stft_dense_layer(h1_stft)#.reshape(h1_stft.shape[0],-1))\n",
    "        if self.midibert_mode:\n",
    "            h2_note = x_symbolic_note_ar\n",
    "        else:\n",
    "            h1_note = self.note_lstm(x_symbolic_note_ar.to(self.device))\n",
    "            h2_note = self.note_dense_layer(h1_note)#.reshape(h1_note.shape[0],-1))\n",
    "        \n",
    "        h1_scalar = self.scalar_mlp(x_scalar.to(self.device))\n",
    "        h2_scalar = self.scalar_dense_layer(h1_scalar)\n",
    "\n",
    "        \n",
    "        input_final_part = torch.cat((h2_stft, h2_note, h2_scalar), dim=1) # sample number stays same\n",
    "        #logits = self.final_part(input_final_part) # predictions for sublimity, ease, vitality\n",
    "        logits1 = self.dim1(input_final_part)\n",
    "        logits2 = self.dim2(input_final_part)\n",
    "        logits3 = self.dim3(input_final_part)\n",
    "        \n",
    "        if not self.label_type_categorical:\n",
    "            logits1 = self.regressive_tanh(logits1)*3\n",
    "            logits2 = self.regressive_tanh(logits2)*3\n",
    "            logits3 = self.regressive_tanh(logits3)*3\n",
    "            \n",
    "        return logits1, logits2, logits3\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Create trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training:\n",
    "    \n",
    "    def __init__(self, full_model, criterion, optimizer, device):\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.full_model = full_model\n",
    "        self.device = device\n",
    "        \n",
    "        self.number_epochs = None\n",
    "        self.train_loss = False\n",
    "        self.val_loss = False\n",
    "        self.test_loss = False\n",
    "        \n",
    "        self.train_acc = False\n",
    "        self.val_acc = False\n",
    "        self.test_acc = False\n",
    "\n",
    "        self.train_r_squared = False\n",
    "        self.val_r_squared = False\n",
    "        self.test_r_squared = False\n",
    "\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def train(self, \n",
    "              train_batches_stft, train_batches_symbolic, train_batches_scalar, train_y, \n",
    "              val_batches_stft, val_batches_symbolic, val_batches_scalar, val_y, \n",
    "              number_epochs):\n",
    "\n",
    "        self.number_epochs = number_epochs\n",
    "        \n",
    "        \n",
    "        # evaluate first:\n",
    "        train_loss, train_acc, train_r_squared = self.evaluate(train_batches_stft, train_batches_symbolic, train_batches_scalar, train_y)\n",
    "        val_loss, val_acc, val_r_squared = self.evaluate(val_batches_stft, val_batches_symbolic, val_batches_scalar, val_y)\n",
    "\n",
    "        train_loss_total = train_loss.reshape(1,-1)\n",
    "        val_loss_total = val_loss.reshape(1,-1)\n",
    "        train_r_squared_total = [train_r_squared]\n",
    "        val_r_squared_total = [val_r_squared]\n",
    "        for epoch_ind in range(number_epochs):\n",
    "\n",
    "            train_loss, train_acc,train_r_squared = self.update(train_batches_stft, train_batches_symbolic, train_batches_scalar, train_y)\n",
    "            val_loss, val_acc, val_r_squared = self.evaluate(val_batches_stft, val_batches_symbolic, val_batches_scalar, val_y)\n",
    "\n",
    "            train_loss_total = torch.cat((train_loss_total,train_loss.reshape(1,-1)),dim=0)\n",
    "            val_loss_total = torch.cat((val_loss_total,val_loss.reshape(1,-1)),dim=0)\n",
    "            train_r_squared_total.append(train_r_squared)\n",
    "            val_r_squared_total.append(val_r_squared)\n",
    "            \n",
    "            \n",
    "        self.train_loss, self.val_loss = train_loss_total[-1], val_loss_total[-1]\n",
    "        self.train_r_squared, self.val_r_squared = train_r_squared_total[-1], val_r_squared_total[-1]\n",
    "        self.train_acc, self.val_acc = train_acc, val_acc\n",
    "\n",
    "        return train_loss_total, val_loss_total,train_r_squared,val_r_squared\n",
    "        \n",
    "    def update(self, batches_stft, batches_symbolic, batches_scalar, y):\n",
    "        \n",
    "        # bring model into training mode:\n",
    "        self.full_model.train()\n",
    "               \n",
    "        c_loss = 0\n",
    "        loss_total = 0\n",
    "        r_squared = 0\n",
    "        rc_loss = 0\n",
    "        counted_correct_pred_total = 0\n",
    "               \n",
    "        for x_stft, x_symbolic_note_ar, x_scalar, y_ in zip(batches_stft, batches_symbolic, batches_scalar, y):\n",
    "        \n",
    "            if y_.shape[1]>3: # we have categorical case\n",
    "                y1 = y_[:,0:2]\n",
    "                y2 = y_[:,2:4]\n",
    "                y3 = y_[:,4:]\n",
    "                act = lambda x: x\n",
    "\n",
    "            else: # regressive y\n",
    "                y1 = y_[:,0].reshape((-1,1))\n",
    "                y2 = y_[:,1].reshape((-1,1))\n",
    "                y3 = y_[:,2].reshape((-1,1))\n",
    "                tanh_ = torch.nn.Tanh()\n",
    "                act = lambda x: tanh_(x)*5 # range [-5, 5] because regressive labels were transfered \n",
    "                    # into z-score\n",
    "\n",
    "            x_stft, x_symbolic_note_ar, x_scalar, y1,y2,y3= x_stft.to(self.device), x_symbolic_note_ar.to(self.device), x_scalar.to(self.device), y1.to(self.device), y2.to(self.device), y3.to(self.device)\n",
    "            \n",
    "            logits1, logits2, logits3 = self.full_model(x_stft, x_symbolic_note_ar, x_scalar)\n",
    "              \n",
    "            logits = torch.cat((logits1, logits2, logits3), dim=1) \n",
    "            y = torch.cat((y1,y2,y3), dim=1) \n",
    "                \n",
    "            loss1 = self.criterion(act(logits1), y1)\n",
    "            loss2 = self.criterion(act(logits2), y2)\n",
    "            loss3 = self.criterion(act(logits3), y3)\n",
    "\n",
    "            loss = loss1 + loss2 + loss3\n",
    "            \n",
    "            c_loss += loss.shape[0]\n",
    "            loss_total += torch.sum(loss, dim=0)\n",
    "\n",
    "\n",
    "            counted_correct_pred = self.my_acc_tool(act(logits), y)\n",
    "            counted_correct_pred_total += counted_correct_pred\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "            if y.shape[0]>1:\n",
    "                rc_loss += 1\n",
    "                r_squared += r2_score(logits, y).item()\n",
    "\n",
    "        avg_loss = loss_total/c_loss\n",
    "        \n",
    "        avg_acc = counted_correct_pred_total/c_loss\n",
    "        r_squared_mean = r_squared/rc_loss\n",
    "        return avg_loss, avg_acc, r_squared_mean\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def my_acc_tool(self, logits, y):\n",
    "\n",
    "        #if 'BCE' in str(self.criterion): # binary labels\n",
    "        if len(set(y.flatten().tolist())) < 3: # binary labels\n",
    "            predictions1 = (self.sig(logits[:,0]) >= 0.5).int().reshape((-1,1))\n",
    "            predictions2 = (self.sig(logits[:,2]) >= 0.5).int().reshape((-1,1))\n",
    "            predictions3 = (self.sig(logits[:,4]) >= 0.5).int().reshape((-1,1))\n",
    "            y1 = y[:,0].reshape((-1,1))\n",
    "            y2 = y[:,2].reshape((-1,1))\n",
    "            y3 = y[:,4].reshape((-1,1))\n",
    "            \n",
    "            predictions = torch.cat((predictions1,predictions2,predictions3),dim=1)\n",
    "            y = torch.cat((y1,y2,y3),dim=1)\n",
    "            \n",
    "        else:\n",
    "            predictions = torch.round(logits, decimals=1)\n",
    "            y = torch.round(y, decimals=1)\n",
    "\n",
    "        wrong_values = abs(predictions-y)\n",
    "        wrong_values[wrong_values > 0] = 1 # we only count correct (0) and incorrect (1) here\n",
    "\n",
    "        wrong_values_sum = torch.sum(wrong_values, dim=0)\n",
    "        counted_correct_pred = (predictions.shape[0] - wrong_values_sum).int()\n",
    "        return counted_correct_pred\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, batches_stft, batches_symbolic, batches_scalar, y, val=True):\n",
    "        \n",
    "        self.full_model.eval()\n",
    "          \n",
    "        c_loss = 0\n",
    "        loss_total = 0\n",
    "        counted_correct_pred_total = 0\n",
    "        r_squared_sum = 0\n",
    "        rc_loss = 0\n",
    "        for x_stft, x_symbolic_note_ar, x_scalar, y_ in  zip(batches_stft, batches_symbolic, batches_scalar, y): # (x,y)=single batch\n",
    "             \n",
    "            if y_.shape[1]>3: # we have categorical case\n",
    "                y1 = y_[:,0:2]\n",
    "                y2 = y_[:,2:4]\n",
    "                y3 = y_[:,4:]\n",
    "                act = lambda x: x\n",
    "                     \n",
    "            else: # regressive y\n",
    "\n",
    "                y1 = y_[:,0].reshape((-1,1))\n",
    "                y2 = y_[:,1].reshape((-1,1))\n",
    "                y3 = y_[:,2].reshape((-1,1))\n",
    "                tanh_ = torch.nn.Tanh()\n",
    "                act = lambda x: tanh_(x)*5 # range [-5, 5] because regressive labels were transfered \n",
    "                    # into z-score*5  \n",
    "                    \n",
    "            x_stft, x_symbolic_note_ar, x_scalar = x_stft.to(self.device), x_symbolic_note_ar.to(self.device), x_scalar.to(self.device)\n",
    "            y1, y2, y3 = y1.to(self.device), y2.to(self.device), y3.to(self.device)\n",
    "            logits1, logits2, logits3 = self.full_model(x_stft, x_symbolic_note_ar, x_scalar)\n",
    "\n",
    "            \n",
    "            logits = torch.cat((logits1, logits2, logits3), dim=1) \n",
    "            y = torch.cat((y1,y2,y3), dim=1) \n",
    "            \n",
    "            loss1 = self.criterion(act(logits1), y1)\n",
    "            loss2 = self.criterion(act(logits2), y2)\n",
    "            loss3 = self.criterion(act(logits3), y3)\n",
    "                        \n",
    "            loss = loss1 + loss2 + loss3\n",
    "            \n",
    "            \n",
    "            c_loss += loss.shape[0]\n",
    "            loss_total += torch.sum(loss, dim=0)\n",
    "                                       \n",
    "            counted_correct_pred = self.my_acc_tool(logits, y)\n",
    "            counted_correct_pred_total += counted_correct_pred\n",
    "\n",
    "            if y.shape[0]>1:\n",
    "                r_squared = r2_score(logits, y)\n",
    "                r_squared_sum += r_squared.item()\n",
    "                rc_loss += 1\n",
    "\n",
    "        avg_loss = loss_total/c_loss\n",
    "        avg_acc = counted_correct_pred_total/c_loss\n",
    "        r_squared_mean = r_squared_sum/rc_loss\n",
    "        if val == False:\n",
    "            self.test_loss = avg_loss    \n",
    "            self.test_acc = avg_acc       \n",
    "            self.test_r_squared = r_squared_mean\n",
    "\n",
    "        return avg_loss, avg_acc, r_squared_mean\n",
    "        \n",
    "    def state_dict(self):\n",
    "\n",
    "        state_dict = {\n",
    "            \"test_loss\": self.test_loss,\n",
    "            \"val_loss\": self.val_loss,\n",
    "            \"train_loss\": self.train_loss,\n",
    "            \n",
    "            \"test_acc\": self.test_acc,\n",
    "            \"val_acc\": self.val_acc,\n",
    "            \"train_acc\": self.train_acc,\n",
    "\n",
    "            \"test_r_squared\": self.test_r_squared,\n",
    "            \"train_r_squared\": self.train_r_squared,\n",
    "            \"val_r_squared\": self.val_r_squared,\n",
    "\n",
    "            \"model\": self.full_model.state_dict(),\n",
    "            \"objective\": self.criterion.state_dict(),\n",
    "            \"optimizer\": self.optimizer,#.state_dict(),\n",
    "            \"number_epochs\": self.number_epochs,\n",
    "            \"criterion\": self.criterion\n",
    "        }\n",
    "        \n",
    "        return state_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Load tensors and MusicBERT model for MIDI files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use convolution to work with the mel scale?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_mode = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use MusicBERT model instead of note array as symbolic track of the multi-modal model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "midibert_mode = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is only needed when '1. Prepare inputs for net' section of this notebook wasn't run to generate the inputs as the inputs already exist in the directory 'created_input_tensors'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = 'created_input_tensors'\n",
    "\n",
    "samples_mel_tensor = torch.load(input_folder+'/mel_tensor.pt')\n",
    "\n",
    "if midibert_mode:\n",
    "    symbolic_input_tensor = torch.load(input_folder+'/midibert_input_tensor.pt')\n",
    "    symbolic_input_tensor.requires_grad = False\n",
    "else:\n",
    "    symbolic_input_tensor = torch.load(input_folder+'/symbolic_input_tensor.pt')\n",
    "\n",
    "scalar_tensor = torch.load(input_folder+'/scalar_tensor.pt')\n",
    "\n",
    "categorical_labels_tensor = torch.load(input_folder+'/categorical_labels_tensor.pt')\n",
    "regres_labels_tensor = torch.load(input_folder+'/regressive_labels_tensor.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([360, 20, 291]) torch.Size([360, 13]) torch.Size([360, 30]) torch.Size([360, 3]) torch.Size([360, 3])\n"
     ]
    }
   ],
   "source": [
    "print(samples_mel_tensor.shape, symbolic_input_tensor.shape, scalar_tensor.shape, categorical_labels_tensor.shape, regres_labels_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reformulate categorical_labels_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hotter(full_y_arr):\n",
    "    for i in range(full_y_arr.shape[1]):\n",
    "        cat_label = full_y_arr[:,i].reshape((-1,1))\n",
    "        reverse = abs(cat_label - torch.ones((full_y_arr.shape[0],1),dtype=torch.int))\n",
    "        yield torch.cat((reverse, cat_label),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_sub, one_hot_vital, one_hot_un = one_hotter(categorical_labels_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_y = torch.cat((one_hot_sub, one_hot_vital, one_hot_un),dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reformulate regressive values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_sub, one_vital, one_un = regres_labels_tensor[:,0],regres_labels_tensor[:,1],regres_labels_tensor[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Create training loop: Unnested CV with Hyperparameter variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set all hyperparameters:\n",
    "# 8748 options\n",
    "lr = [1e-5, 1e-3, 1e-5]\n",
    "batch_size = [16, 4, 32]\n",
    "number_epochs = [800]\n",
    "\n",
    "# optimizer:\n",
    "betas = [(0.98, 0.999)] \n",
    "\n",
    "# hyperparameters for network architecture of scalar features dataset:\n",
    "hidden_dim_scalar = [50, 500, 750]\n",
    "out_dim_scalar = [10]\n",
    "\n",
    "kernel_size_stft=[(3,3),(3,5)]\n",
    "channel_size_stft=[3,6,9]\n",
    "# given by Tripathi, Tripathi, Beigi paper:\n",
    "hidden_dim_stft=[56]\n",
    "out_dim_stft=[0]\n",
    "bi_direct_stft=[True]\n",
    "\n",
    "\n",
    "\n",
    "dense_out_stft= [5, 10, 100] \n",
    "if midibert_mode:\n",
    "    dense_out_note= [13]\n",
    "    hidden_dim_note=[56]\n",
    "    out_dim_note=[0]\n",
    "    bi_direct_note=[True]\n",
    "else:\n",
    "    dense_out_note= [10, 20, 56] \n",
    "    hidden_dim_note=[56]\n",
    "    out_dim_note=[0]\n",
    "    bi_direct_note=[True]\n",
    "    \n",
    "dense_out_scalar= [20, 30, 200] \n",
    "\n",
    "final_hidden_dense_out = [50, 100, 500] \n",
    "\n",
    "# set the label to categorical or regressive:\n",
    "label_type_categorical = [False, True]\n",
    "\n",
    "device = ['cuda'] if torch.cuda.is_available() else ['cpu']\n",
    "\n",
    "# create dictionary which is filled with all hyperparameters:\n",
    "global hyper_dict\n",
    "hyper_dict = {}\n",
    "\n",
    "def fill_hyper_dict(*var):\n",
    "    \n",
    "    global_variables = globals()\n",
    "    var = list(var)\n",
    "    var_id_list = [id(var_ele) for var_ele in var]\n",
    "    \n",
    "    for name, value in global_variables.items():\n",
    "        \n",
    "        if id(value) in var_id_list:\n",
    "            found_ind = var_id_list.index(id(value))\n",
    "            hyper_dict[name] = var[found_ind]\n",
    "            \n",
    "            var_id_list.pop(found_ind)\n",
    "            var.pop(found_ind)\n",
    "\n",
    "        if len(var_id_list)==0:\n",
    "            return None\n",
    "        \n",
    "    return None\n",
    "\n",
    "fill_hyper_dict(lr, batch_size, number_epochs,\n",
    "               betas,\n",
    "               hidden_dim_scalar, out_dim_scalar,\n",
    "               hidden_dim_stft, out_dim_stft, bi_direct_stft, kernel_size_stft, channel_size_stft,\n",
    "               hidden_dim_note, out_dim_note, bi_direct_note,\n",
    "               dense_out_stft, \n",
    "               dense_out_note,\n",
    "               final_hidden_dense_out,\n",
    "               dense_out_scalar, \n",
    "               label_type_categorical,\n",
    "               device)\n",
    "\n",
    "# combine all hyperparameter options with each other by cartesian product/create grid:\n",
    "hyper_cartesian_prod = [dict(zip(hyper_dict.keys(), values)) for values in itertools.product(*hyper_dict.values())]\n",
    "#hyper_cartesian_prod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Actual training loop with cross validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start hyper choice loop 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/c/anaconda3/envs/multimodal_env/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/c/anaconda3/envs/multimodal_env/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/c/anaconda3/envs/multimodal_env/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/c/anaconda3/envs/multimodal_env/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# text file in which the loss scores of model choices are saved:\n",
    "txt_title = 'scores_of_best_hyperparameter_choices_20230901.txt'\n",
    "\n",
    "# create dataloader with same sampling/shuffling mode for each dataset:\n",
    "g = torch.Generator()\n",
    "g.manual_seed(19923286)\n",
    "\n",
    "\n",
    "# split datasets into train, test and validation sets:\n",
    "kfold_test = StratifiedKFold(n_splits=5, shuffle=True, random_state=np.random.seed(19923286))\n",
    "kfold_val = StratifiedKFold(n_splits=5, shuffle=True, random_state=np.random.seed(19923286))\n",
    "\n",
    "# create folder with best models:\n",
    "folder = 'best_multimodal_models'\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "\n",
    "for hyper_ind, hyper_choice in enumerate(hyper_cartesian_prod):\n",
    "        \n",
    "    print(f'start hyper choice loop {hyper_ind}...')\n",
    "    best_test_loss_in_hyper_scenario = float('inf')\n",
    "    if hyper_choice['label_type_categorical']==True:\n",
    "        y_split1 = categorical_labels_tensor[:,2].squeeze()\n",
    "    else:\n",
    "        y_split1 = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='quantile').fit_transform(regres_labels_tensor[:,2].reshape(-1,1)).squeeze()\n",
    "    for rest_id, test_id in kfold_test.split(scalar_tensor,y_split1):\n",
    "\n",
    "        X_symbolic_rest, X_symbolic_test = symbolic_input_tensor[rest_id], symbolic_input_tensor[test_id]\n",
    "        X_mel_rest, X_mel_test = samples_mel_tensor[rest_id], samples_mel_tensor[test_id]\n",
    "        X_scalar_rest, X_scalar_test = scalar_tensor[rest_id], scalar_tensor[test_id]\n",
    "\n",
    "        if hyper_choice['label_type_categorical']==True:\n",
    "            \n",
    "            y_rest, y_test = one_hot_y[rest_id], one_hot_y[test_id]\n",
    "\n",
    "            y_rest = y_rest.type(torch.float32)\n",
    "            y_test = y_test.type(torch.float32)\n",
    "\n",
    "        else:\n",
    "            \n",
    "            y_rest, y_test = regres_labels_tensor[rest_id], regres_labels_tensor[test_id]\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        if hyper_choice['label_type_categorical']==True:\n",
    "            y_split2 = categorical_labels_tensor[:,2][rest_id].squeeze()\n",
    "        else:\n",
    "            y_split2 = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='quantile').fit_transform(y_rest[:,2].reshape(-1,1)).squeeze()\n",
    "        for train_id, val_id in kfold_val.split(X_scalar_rest, y_split2):\n",
    "\n",
    "            X_symbolic_train, X_symbolic_val = X_symbolic_rest[train_id], X_symbolic_rest[val_id]\n",
    "            X_mel_train, X_mel_val =  X_mel_rest[train_id],  X_mel_rest[val_id]\n",
    "            X_scalar_train, X_scalar_val = X_scalar_rest[train_id], X_scalar_rest[val_id]\n",
    "\n",
    "            y_train, y_val = y_rest[train_id], y_rest[val_id]\n",
    "\n",
    "            # create dataloaders:\n",
    "            s_train = torch.utils.data.RandomSampler(X_symbolic_train, generator=g)\n",
    "            s_val = torch.utils.data.RandomSampler(X_symbolic_val, generator=g)\n",
    "\n",
    "            X_symbolic_trainloader = torch.utils.data.DataLoader(X_symbolic_train, batch_size=hyper_choice['batch_size'], sampler=s_train,num_workers=4)\n",
    "            X_symbolic_valloader = torch.utils.data.DataLoader(X_symbolic_val, batch_size=hyper_choice['batch_size'], sampler=s_val,num_workers=4)\n",
    "\n",
    "            X_mel_trainloader = torch.utils.data.DataLoader(X_mel_train, batch_size=hyper_choice['batch_size'], sampler=s_train,num_workers=4)\n",
    "            X_mel_valloader = torch.utils.data.DataLoader(X_mel_val, batch_size=hyper_choice['batch_size'], sampler=s_val, num_workers=4)\n",
    "\n",
    "            X_scalar_trainloader = torch.utils.data.DataLoader(X_scalar_train, batch_size=hyper_choice['batch_size'], sampler=s_train, num_workers=4)\n",
    "            X_scalar_valloader = torch.utils.data.DataLoader(X_scalar_val, batch_size=hyper_choice['batch_size'], sampler=s_val, num_workers=4)\n",
    "\n",
    "            y_trainloader = torch.utils.data.DataLoader(y_train, batch_size=hyper_choice['batch_size'], sampler=s_train, num_workers=4)\n",
    "            y_valloader = torch.utils.data.DataLoader(y_val, batch_size=hyper_choice['batch_size'], sampler=s_val, num_workers=4)\n",
    "\n",
    "            # initialize model:\n",
    "            if midibert_mode:\n",
    "                time_note = 1\n",
    "            else: \n",
    "                time_note = X_symbolic_train.shape[2]\n",
    "            \n",
    "            full_model = FullModel(conv_mode=conv_mode, num_feat_stft=X_mel_train.shape[1], time_stft=X_mel_train.shape[2], hidden_dim_stft=hyper_choice['hidden_dim_stft'], out_dim_stft=hyper_choice['out_dim_stft'], bi_direct_stft=hyper_choice['bi_direct_stft'],\n",
    "                     # num_feat_stft: the rows represent one bin/time window (features per sample)\n",
    "                     kernel_size_stft=hyper_choice['kernel_size_stft'], channel_size_stft=hyper_choice['channel_size_stft'],\n",
    "                     midibert_mode=midibert_mode, num_feat_note=X_symbolic_train.shape[1], time_note=time_note, hidden_dim_note=hyper_choice['hidden_dim_note'], out_dim_note=hyper_choice['out_dim_note'], bi_direct_note=hyper_choice['bi_direct_note'],\n",
    "                     # num_feat_note: the rows represent one time part (features per sample)\n",
    "                     dense_out_stft=hyper_choice['dense_out_stft'], \n",
    "                     dense_out_note=hyper_choice['dense_out_note'],\n",
    "                     final_hidden_dense_out=hyper_choice['final_hidden_dense_out'],\n",
    "                     num_feat_scalar=X_scalar_train.shape[-1], hidden_dim_scalar=hyper_choice['hidden_dim_scalar'], out_dim_scalar=hyper_choice['out_dim_scalar'],\n",
    "                     dense_out_scalar=hyper_choice['dense_out_scalar'],\n",
    "                     device=hyper_choice['device'],\n",
    "                     label_type_categorical=hyper_choice['label_type_categorical'])\n",
    "\n",
    "            # start training (evaluation and optimization):\n",
    "            # loss criterion:\n",
    "            if hyper_choice['label_type_categorical']==True:\n",
    "                # multilabel classification: https://machinelearningmastery.com/multi-label-classification-with-deep-learning/\n",
    "                criterion = nn.BCEWithLogitsLoss(reduction=\"none\").to(hyper_choice['device']) #, nn.CrossEntropyLoss(reduction='mean').to(config.dev) # BinaryCrossEntropy\n",
    "            else:\n",
    "                criterion = nn.MSELoss(reduction=\"none\").to(hyper_choice['device']) #, nn.CrossEntropyLoss(reduction='mean').to(config.dev)\n",
    "\n",
    "            # Optimizer:\n",
    "            optimizer = torch.optim.Adam(full_model.parameters(), lr=hyper_choice['lr'], betas=hyper_choice['betas'], eps=1e-08, weight_decay=0, amsgrad=False, foreach=None, maximize=False, capturable=False, differentiable=False, fused=None)\n",
    "\n",
    "            # training:\n",
    "            my_trainer = Training(full_model=full_model, criterion=criterion, optimizer=optimizer, device=hyper_choice['device'])\n",
    "            train_loss_list, val_loss_list, train_r_squared, val_r_squared = my_trainer.train(train_batches_stft=X_mel_trainloader, train_batches_symbolic=X_symbolic_trainloader, train_batches_scalar=X_scalar_trainloader, train_y=y_trainloader, \n",
    "                  val_batches_stft=X_mel_valloader, val_batches_symbolic=X_symbolic_valloader, val_batches_scalar=X_scalar_valloader, val_y=y_valloader, \n",
    "                  number_epochs=hyper_choice['number_epochs'])\n",
    "\n",
    "            if val_loss_list[-1].mean() < best_val_loss:\n",
    "\n",
    "                best_val_loss = val_loss_list[-1].mean()\n",
    "                best_state_dict = my_trainer.state_dict()\n",
    "                best_model = full_model\n",
    "\n",
    "        # evaluate performance on test set:\n",
    "        # dataloader on test set:\n",
    "        s_test = torch.utils.data.RandomSampler(X_symbolic_test, generator=g)\n",
    "\n",
    "        X_symbolic_testloader = torch.utils.data.DataLoader(X_symbolic_test, batch_size=hyper_choice['batch_size'], sampler=s_test, num_workers=4)\n",
    "        X_mel_testloader = torch.utils.data.DataLoader(X_mel_test, batch_size=hyper_choice['batch_size'], sampler=s_test, num_workers=4)\n",
    "        X_scalar_testloader = torch.utils.data.DataLoader(X_scalar_test, batch_size=hyper_choice['batch_size'], sampler=s_test, num_workers=4)\n",
    "        y_testloader = torch.utils.data.DataLoader(y_test, batch_size=hyper_choice['batch_size'], sampler=s_test, num_workers=4)\n",
    "\n",
    "        my_trainer_test = Training(full_model=best_model, criterion=best_state_dict['criterion'], optimizer=None, device=hyper_choice['device'])\n",
    "\n",
    "        test_loss, test_acc, test_r_squared = my_trainer_test.evaluate(batches_stft=X_mel_testloader, batches_symbolic=X_symbolic_testloader, batches_scalar=X_scalar_testloader, y=y_testloader, val=False)\n",
    "        if test_loss.mean() < best_test_loss_in_hyper_scenario:\n",
    "\n",
    "            best_test_loss_in_hyper_scenario = test_loss.mean()\n",
    "\n",
    "            # recreate state dict:\n",
    "            my_trainer_test.val_loss = best_state_dict['val_loss']\n",
    "            my_trainer_test.train_loss = best_state_dict['train_loss']\n",
    "\n",
    "            my_trainer_test.val_r_squared = best_state_dict['val_r_squared']\n",
    "            my_trainer_test.train_r_squared = best_state_dict['train_r_squared']\n",
    "\n",
    "            my_trainer_test.val_acc = best_state_dict['val_acc']\n",
    "            my_trainer_test.train_acc = best_state_dict['train_acc']\n",
    "\n",
    "            my_trainer_test.objective = best_state_dict['objective']\n",
    "            my_trainer_test.optimizer = best_state_dict['optimizer']\n",
    "            my_trainer_test.number_epochs = best_state_dict['number_epochs']\n",
    "\n",
    "            best_state_dict_test = my_trainer_test.state_dict()\n",
    "\n",
    "            print('better model found')\n",
    "\n",
    "    # store best test loss in each combination of hyper-parameter scenario:    \n",
    "\n",
    "\n",
    "    if True:\n",
    "        testl = best_state_dict_test['test_loss']\n",
    "        testa = best_state_dict_test['test_acc'] \n",
    "        testr = best_state_dict_test['test_r_squared'] \n",
    "        print('test acc:', testa, 'test r', testr)\n",
    "\n",
    "        save_check = False\n",
    "        if hyper_choice['label_type_categorical']==True:\n",
    "        \n",
    "            b = testa.clone().detach()\n",
    "            max_ = torch.max(b).item()\n",
    "            if max_ >= 0.8:\n",
    "                save_check = True \n",
    "            elif max_ >= 0.6:\n",
    "                max_ind = torch.argmax(b).item()\n",
    "                b[max_ind] = 0\n",
    "                max2_ = torch.max(b).item()\n",
    "                if max2_ >= 0.60:\n",
    "                    save_check = True\n",
    "        \n",
    "        elif (hyper_choice['label_type_categorical']==False and (testl.item() < 2.1 or torch.any(testa>0.1).item()) or testr>=-1):\n",
    "            save_check = True\n",
    "       \n",
    "        if save_check==True:\n",
    "        \n",
    "            path_to_store = folder + f'/multimodal_model_dict_hyper_round{hyper_ind}'\n",
    "            torch.save(best_state_dict_test, path_to_store + '.pth')\n",
    "    \n",
    "            with open(txt_title, 'a') as f:\n",
    "\n",
    "                tl = best_state_dict_test['train_loss'].tolist()#.item()\n",
    "                vl = best_state_dict_test['val_loss'].tolist()#.item()\n",
    "                testl = testl.tolist()\n",
    "                ta = best_state_dict_test['train_acc'].tolist()#.item()\n",
    "                va = best_state_dict_test['val_acc'].tolist()#.item()\n",
    "                testa = testa.tolist()\n",
    "\n",
    "                tr = best_state_dict_test[\"train_r_squared\"]\n",
    "                vr = best_state_dict_test[\"val_r_squared\"]\n",
    "                testr = best_state_dict_test[\"test_r_squared\"]\n",
    "\n",
    "                f.write(f'############# Hyper ind: {hyper_ind}; categorical: {hyper_choice[\"label_type_categorical\"]} ##############')\n",
    "                f.write(f'test accuracy: {testa}, test loss: {testl}, r^2 : {testr}')\n",
    "                f.write(str(hyper_choice))\n",
    "                f.write(f\"train acc: {ta}, val acc: {va}, train loss: {tl}, val loss: {vl}, r^2 train: {tr}, r^2 val: {vr}\\n\\n\")\n",
    "\n",
    "                \n",
    "    print('...done hyper choice loop')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('NOTEBOOK FINISHED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
